# Linear-Optimizers-From-Scratch
Implementing GD, Momentum based GD, NAG, AdaGrad, RMSProp and Adam Optimizers from scratch with python.
